[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "From Databricks to Power BI",
    "section": "",
    "text": "Everything you need to know about importing data from Databricks to Power BI.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/first_blog_post/post.html",
    "href": "posts/first_blog_post/post.html",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Mastering Your Workflow: A Data Analyst’s Roadmap to Success\nIn the world of data analysis, having a solid workflow isn’t just a nice-to-have—it’s your secret weapon for delivering exceptional results. A well-crafted workflow helps you:\n\nSet crystal-clear expectations for your project\nDefine and measure key outcomes with precision\nBoost your accuracy and efficiency\nConsistently deliver high-quality, insightful work\n\nEvery project is unique, and so is every workflow. But one thing remains constant: having a structured approach is always better than flying blind.\n\n\n1. Understanding the Business Case\nBefore diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n2. Measurement Planning\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n3. Collect and Prep Data\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n4. Understand the Data\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity -Composition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n5. Analyze and Visualize\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n6. Develop Data-driven Insights\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n7. Measure, Test, Optimize\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities\n\n\nRemember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value. By embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\n\n\n\n\n\nCloud\n\n\nConcepts\n\n\n\nA deep dive into why everyone is moving to the cloud and why you should consider it too.\n\n\n\n\n\nMar 5, 2025\n\n\nSarah Dorny\n\n\n\n\n\n\n\n\n\n\n\n\nBehind the Scenes: My Senior Project on Cloud Data Migration\n\n\n\n\n\n\nProject\n\n\n\nA brief overview of my senior project: a migration of an on-prem database to the cloud.\n\n\n\n\n\nFeb 10, 2025\n\n\nSarah Dorny\n\n\n\n\n\n\n\n\n\n\n\n\nIs A Career in Data Right For You?\n\n\n\n\n\n\nPersonal\n\n\n\nLearn more about what a career in with data looks like and if it is right for you!.\n\n\n\n\n\nJan 25, 2022\n\n\nSarah Dorny\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Sarah Dorny.",
    "section": "",
    "text": "I have two years of experience working in a Data Analyst capacity. Detail-oriented Data Scientist with over 2 years of industry experience in business intelligence and project management. Skilled in data extraction, modeling, and visualization to deliver actionable insights. Microsoft Power BI certified (PL-300)."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Why Teamwork Matters",
    "section": "",
    "text": "“If everyone is moving forward together, then success takes care of itself.” - Henry Ford"
  },
  {
    "objectID": "posts/difference/index.html",
    "href": "posts/difference/index.html",
    "title": "Why Migrate to the Cloud?",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\nIntroduction\nCloud computing is no longer a foreign concept - we interact with it daily through services like Google Drive, Dropbox, and Office 365. These familiar tools represent just the surface of what cloud technology offers to businesses and organizations today.\n\n\n\nWhat is Cloud Computing?\nCloud computing delivers technology services—including computing power, storage, databases, and networking—over the internet with a flexible pay-as-you-go pricing model. This approach fundamentally changes how organizations acquire and manage their IT resources.\n\n\nStrategic Advantages of Cloud Migration\n\nOperational Excellence\n\nScalability: Dynamically adjust resources to match demand without capital expenditure on hardware\n\nSpeed: Deploy new applications and services in minutes rather than weeks or months\n\nReliability: Distribute data across multiple geographic regions, eliminating single points of failure\n\nSecurity: Leverage enterprise-grade security measures and compliance certifications from major providers\n\n\n\nBusiness Impact\n\nCost Efficiency: Convert capital expenses to operational expenses with predictable pricing models\n\nInnovation Acceleration: Access cutting-edge technologies without significant upfront investment\n\nGlobal Reach: Deploy resources closer to end-users worldwide with minimal effort\n\nFocus on Core Business: Reduce time spent managing infrastructure to concentrate on strategic initiatives\n\n\n\n\nService Models Explained\nCloud computing offers multiple service models that provide different levels of control and responsibility:\n\n\n\n\n\n\n\n\n\nModel\nTraditional Analogy\nWhat You Manage\nWhat Provider Manages\n\n\n\n\nOn-Premises\nOwning a car\nEverything (hardware, software, maintenance)\nNothing\n\n\nIaaS (Infrastructure as a Service)\nRenting a car\nOS, applications, data\nHardware, networking\n\n\nPaaS (Platform as a Service)\nUsing rideshare (Uber)\nApplications, data\nHardware, OS, middleware\n\n\nSaaS (Software as a Service)\nTaking public transit\nData and access\nEverything else\n\n\n\n\n\nImplementation Roadmap\n\n1. Select a Cloud Deployment Model\n\nPublic Cloud: Resources owned and operated by third-party providers\n\nPrivate Cloud: Dedicated resources for a single organization\n\nHybrid Cloud: Combination of public and private clouds with orchestration between them\n\nMulti-Cloud: Strategic use of multiple cloud providers for different services\n\n\n\n2. Develop Necessary Expertise\n\nRoles to Consider: Cloud architects, cloud engineers, DevOps specialists, security experts\n\nSkills to Acquire: Cloud provider certifications, infrastructure as code, containerization, orchestration\n\n\n\n3. Choose Appropriate Provider(s)\n\nMajor Players: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)\n\nSelection Criteria: Service offerings, pricing models, geographic presence, integration capabilities\n\n\n\n4. Create a Comprehensive Migration Strategy\n\nAssessment: Inventory current applications and infrastructure\n\nPlanning: Determine migration approach (rehost, refactor, rearchitect, rebuild)\n\nExecution: Implement the migration with minimal disruption\n\nOptimization: Continuously improve cloud resources and configurations\n\n\n\n\n\nConclusion\nCloud migration has evolved from a technological trend to a business imperative. The combination of financial benefits, operational improvements, and innovation potential makes cloud adoption essential for organizations seeking to remain competitive in the digital landscape.\nBy strategically implementing cloud computing services, companies can enhance agility, improve efficiency, and accelerate innovation while simultaneously reducing technical debt and operational costs. This transformation allows organizations to redirect focus toward their core business objectives and customer needs rather than infrastructure management.\nThe journey to the cloud requires careful planning and execution, but the long-term benefits far outweigh the initial investment in time and resources. As demonstrated in my project, a well-designed migration strategy provides a clear pathway to realizing the full potential of cloud computing."
  },
  {
    "objectID": "posts/difference/index.html#data-engineers-building-the-foundation",
    "href": "posts/difference/index.html#data-engineers-building-the-foundation",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Engineers are the architects who design, build, and maintain the infrastructure that allows data to flow throughout an organization. They create the pipelines that transport data from various sources to storage systems where it can be accessed and analyzed.\n\n\n\n\nProgramming: Proficiency in languages like Python, Java, or Scala\nDatabase Systems: Expert knowledge of SQL and NoSQL databases\nBig Data Technologies: Experience with tools like Hadoop, Spark, and Kafka\nETL Processes: Building robust Extract, Transform, Load pipelines\nCloud Platforms: Working with AWS, Azure, or Google Cloud services\n\n\n\n\nA typical day for a Data Engineer might involve:\n- Designing database schemas\n- Optimizing data flows for performance\n- Implementing data quality checks\n- Setting up data warehousing solutions\n- Creating APIs for data access\n\n\n\nMost Data Engineers come from software engineering or computer science backgrounds and have strong technical foundations in system architecture."
  },
  {
    "objectID": "posts/difference/index.html#data-analysts-interpreting-the-present",
    "href": "posts/difference/index.html#data-analysts-interpreting-the-present",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Analysts focus on examining existing data to answer specific business questions and provide actionable insights. They translate data into information that guides decision-making.\n\n\n\n\nStatistical Analysis: Understanding descriptive statistics\nData Visualization: Creating charts and dashboards\nSQL: Writing queries to extract and manipulate data\nBusiness Intelligence Tools: Proficiency with Tableau, Power BI, or Looker\nSpreadsheet Mastery: Advanced Excel or Google Sheets techniques\n\n\n\n\nA typical day for a Data Analyst might involve: - Creating performance reports - Building interactive dashboards - Conducting A/B test analysis - Identifying trends in customer behavior - Presenting findings to stakeholders\n\n\n\nData Analysts often come from diverse backgrounds including business, economics, mathematics, or even self-taught paths with strong analytical mindsets."
  },
  {
    "objectID": "posts/difference/index.html#data-scientists-predicting-the-future",
    "href": "posts/difference/index.html#data-scientists-predicting-the-future",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Scientists combine advanced statistical methods, machine learning, and domain expertise to extract deeper insights, make predictions, and solve complex problems with data.\n\n\n\n\nAdvanced Statistics: Hypothesis testing, experimental design\nMachine Learning: Building predictive models\nProgramming: Strong coding skills in Python or R\nData Storytelling: Communicating complex findings\nDomain Expertise: Understanding business context\n\n\n\n\nA typical day for a Data Scientist might involve: - Developing predictive algorithms - Training and evaluating ML models - Conducting exploratory data analysis - Testing hypotheses with statistical methods - Researching new analytical approaches\n\n\n\nData Scientists typically have advanced degrees in quantitative fields like statistics, mathematics, computer science, or specialized data science programs."
  },
  {
    "objectID": "posts/difference/index.html#how-these-roles-interact",
    "href": "posts/difference/index.html#how-these-roles-interact",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "In an efficient data organization, these three roles form a symbiotic relationship:\n\nData Engineers build the infrastructure and pipelines that collect and organize raw data\nData Scientists leverage the same infrastructure to build models that predict future outcomes\nData Analysts use this data to explain current business performance and trends"
  },
  {
    "objectID": "posts/difference/index.html#career-progression",
    "href": "posts/difference/index.html#career-progression",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "It’s worth noting that career paths in data can be fluid:\n\nSome Data Analysts develop programming and ML skills to become Data Scientists\nSome Data Scientists move into Data Engineering to better understand the full data stack\nOthers specialize further within their domain, becoming experts in specific industries or techniques"
  },
  {
    "objectID": "posts/difference/index.html#salary-comparisons-as-of-2024",
    "href": "posts/difference/index.html#salary-comparisons-as-of-2024",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Role\nEntry-Level\nMid-Career\nSenior\n\n\n\n\nData Engineer\n$85,000-$110,000\n$110,000-$140,000\n$140,000-$180,000+\n\n\nData Analyst\n$65,000-$85,000\n$85,000-$115,000\n$115,000-$145,000+\n\n\nData Scientist\n$90,000-$120,000\n$120,000-$150,000\n$150,000-$200,000+\n\n\n\n*Note: Salaries vary significantly based on location, industry, company size, and individual experience."
  },
  {
    "objectID": "posts/difference/index.html#conclusion",
    "href": "posts/difference/index.html#conclusion",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "While all three roles work with data, they serve complementary functions in the data value chain:\n\nData Engineers focus on the infrastructure that makes data available\nData Scientists focus on models that predict what will happen next\nData Analysts focus on the insights that explain what’s happening now\n\nUnderstanding these distinctions can help organizations build balanced data teams and help individuals chart their career paths in this rapidly evolving field."
  },
  {
    "objectID": "posts/teamwork/index.html",
    "href": "posts/teamwork/index.html",
    "title": "Project Overview",
    "section": "",
    "text": "Our organization recently completed a strategic data platform migration from an on-premise SQL Server environment to a modern cloud-based analytics ecosystem centered on Azure Databricks. This transformation has enabled us to leverage advanced data processing capabilities, implement a robust data lakehouse architecture, and deliver more powerful business insights through enhanced visualization.\n\n\n\nOur solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface\n\n\n\n\nThe end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information\n\n\n\n\n\nThis migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models\n\n\n\n\n\nOur implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance\n\n\n\n\nThe successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/workflow/post.html",
    "href": "posts/workflow/post.html",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "In the world of data analysis, having a solid workflow isn’t just a nice-to-have—it’s your secret weapon for delivering exceptional results. A well-crafted workflow helps you:\n\nSet crystal-clear expectations for your project\nDefine and measure key outcomes with precision\nBoost your accuracy and efficiency\nConsistently deliver high-quality, insightful work\n\nEvery project is unique, and so is every workflow. But one thing remains constant: having a structured approach is always better than flying blind.\n\n\n\n\n\nBefore diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity\nComposition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities\n\n\n\n\n\nLet’s walk through this workflow with a practical example: analyzing customer churn for an e-commerce company.\n\n\nScenario: The marketing director approaches you concerned about declining customer retention rates.\nKey Questions Addressed:\n- Business goal: Reduce customer churn by 15% within 6 months\n- Stakeholders: Marketing, Sales, and Product teams\n- Motivation: Customer acquisition costs are 5x higher than retention costs\n- Strategic alignment: Company’s annual goal to increase customer lifetime value\n\n\n\nDefined Success Metrics:\n- Primary KPI: 30-day customer churn rate\n- Secondary KPIs: Repeat purchase rate, average time between purchases, cart abandonment rate\n- Data needed: Transaction history, customer demographics, website behavior, support interactions\n\n\n\nActions Taken:\n- Extracted 18 months of transaction data from the CRM\n- Merged with customer profile data from the marketing database\n- Cleaned inconsistent date formats and duplicate customer IDs\n- Created feature for “days since last purchase” and “purchase frequency”\n- Automated weekly data refresh using SQL stored procedures\n\n\n\nKey Discoveries:\n- Data covered 45,000 customers and 120,000 transactions\n- Granularity: Individual transaction level with timestamps\n- Identified seasonal purchasing patterns\n- Recognized that mobile app users had different behavior patterns than website users\n\n\n\nApproach:\n- Created cohort analysis showing retention rates by acquisition channel\n- Developed a churn prediction model using purchase recency, frequency, and monetary value\n- Visualized customer journey maps highlighting dropout points\n- Built an interactive dashboard showing churn trends by customer segment\n\n\n\nKey Findings:\n- 64% of churned customers abandoned their shopping cart in their last session\n- Customers who contacted support were 3x more likely to churn\n- First-time customers who purchased during a major sale had 40% higher churn rates\n- Recommendation: Implement targeted retention campaigns for high-risk segments with personalized offers based on previous purchase history\n\n\n\nResults:\n- A/B tested email re-engagement campaigns with 10% of at-risk customers\n- Most effective campaign reduced churn by 22% in the test group\n- Full implementation projected to save $250,000 annually in customer acquisition costs\n- Continuing to refine customer segmentation based on new behavioral data\n\n\n\n\nRemember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value.\nBy embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change. The e-commerce churn example demonstrates how following a structured workflow transforms raw data into valuable business outcomes—precisely what separates exceptional analysts from the rest."
  },
  {
    "objectID": "posts/workflow/post.html#the-7-step-data-analyst-workflow",
    "href": "posts/workflow/post.html#the-7-step-data-analyst-workflow",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Before diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity\nComposition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities"
  },
  {
    "objectID": "posts/workflow/post.html#real-world-example-e-commerce-customer-churn-analysis",
    "href": "posts/workflow/post.html#real-world-example-e-commerce-customer-churn-analysis",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Let’s walk through this workflow with a practical example: analyzing customer churn for an e-commerce company.\n\n\nScenario: The marketing director approaches you concerned about declining customer retention rates.\nKey Questions Addressed:\n- Business goal: Reduce customer churn by 15% within 6 months\n- Stakeholders: Marketing, Sales, and Product teams\n- Motivation: Customer acquisition costs are 5x higher than retention costs\n- Strategic alignment: Company’s annual goal to increase customer lifetime value\n\n\n\nDefined Success Metrics:\n- Primary KPI: 30-day customer churn rate\n- Secondary KPIs: Repeat purchase rate, average time between purchases, cart abandonment rate\n- Data needed: Transaction history, customer demographics, website behavior, support interactions\n\n\n\nActions Taken:\n- Extracted 18 months of transaction data from the CRM\n- Merged with customer profile data from the marketing database\n- Cleaned inconsistent date formats and duplicate customer IDs\n- Created feature for “days since last purchase” and “purchase frequency”\n- Automated weekly data refresh using SQL stored procedures\n\n\n\nKey Discoveries:\n- Data covered 45,000 customers and 120,000 transactions\n- Granularity: Individual transaction level with timestamps\n- Identified seasonal purchasing patterns\n- Recognized that mobile app users had different behavior patterns than website users\n\n\n\nApproach:\n- Created cohort analysis showing retention rates by acquisition channel\n- Developed a churn prediction model using purchase recency, frequency, and monetary value\n- Visualized customer journey maps highlighting dropout points\n- Built an interactive dashboard showing churn trends by customer segment\n\n\n\nKey Findings:\n- 64% of churned customers abandoned their shopping cart in their last session\n- Customers who contacted support were 3x more likely to churn\n- First-time customers who purchased during a major sale had 40% higher churn rates\n- Recommendation: Implement targeted retention campaigns for high-risk segments with personalized offers based on previous purchase history\n\n\n\nResults:\n- A/B tested email re-engagement campaigns with 10% of at-risk customers\n- Most effective campaign reduced churn by 22% in the test group\n- Full implementation projected to save $250,000 annually in customer acquisition costs\n- Continuing to refine customer segmentation based on new behavioral data"
  },
  {
    "objectID": "posts/workflow/post.html#conclusion",
    "href": "posts/workflow/post.html#conclusion",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Remember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value.\nBy embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change. The e-commerce churn example demonstrates how following a structured workflow transforms raw data into valuable business outcomes—precisely what separates exceptional analysts from the rest."
  },
  {
    "objectID": "posts/teamwork/index.html#executive-summary",
    "href": "posts/teamwork/index.html#executive-summary",
    "title": "Project Overview",
    "section": "",
    "text": "Our organization recently completed a strategic data platform migration from an on-premise SQL Server environment to a modern cloud-based analytics ecosystem centered on Azure Databricks. This transformation has enabled us to leverage advanced data processing capabilities, implement a robust data lakehouse architecture, and deliver more powerful business insights through enhanced visualization."
  },
  {
    "objectID": "posts/teamwork/index.html#project-architecture",
    "href": "posts/teamwork/index.html#project-architecture",
    "title": "Project Overview",
    "section": "",
    "text": "Our solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface"
  },
  {
    "objectID": "posts/teamwork/index.html#data-flow-and-processing",
    "href": "posts/teamwork/index.html#data-flow-and-processing",
    "title": "Project Overview",
    "section": "",
    "text": "The end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information"
  },
  {
    "objectID": "posts/teamwork/index.html#business-benefits",
    "href": "posts/teamwork/index.html#business-benefits",
    "title": "Project Overview",
    "section": "",
    "text": "This migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models"
  },
  {
    "objectID": "posts/teamwork/index.html#technical-implementation-highlights",
    "href": "posts/teamwork/index.html#technical-implementation-highlights",
    "title": "Project Overview",
    "section": "",
    "text": "Our implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance"
  },
  {
    "objectID": "posts/teamwork/index.html#conclusion",
    "href": "posts/teamwork/index.html#conclusion",
    "title": "Project Overview",
    "section": "",
    "text": "The successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/project overview/index.html",
    "href": "posts/project overview/index.html",
    "title": "Project Overview",
    "section": "",
    "text": "Behind the Scenes: My Senior Project on Cloud Data Migration\n\n\nIntroduction\nDid you know that behind every analysis, dashboard, and report lies an intricate process of data wrangling? Before any insights can be drawn, data must first be collected, cleaned, and formatted appropriately. In the industry, this crucial work falls to Data Engineers – the unsung heroes who extract data from various sources, transform it through complex processes, and deliver it in a usable state to Data Scientists and Analysts. Without these engineers laying the groundwork, data analysis simply couldn’t happen.\nYou might be wondering, “What does this data engineering work actually look like in practice?” That’s exactly what this blog post will show you.\nMy senior project tackles a scenario that Data Engineers commonly face: migrating an existing data infrastructure to the cloud. If terms like “data migration” and “cloud infrastructure” sound like technical jargon, don’t worry – I’ve written another article called “Why Migrate to the Cloud?” that explains these concepts in more detail.\nThrough this project, I hope to illuminate a vital but often overlooked aspect of the data world and highlight the essential role that Data Engineers play in enabling modern analytics.\n\n\nInspiration\nThe inspiration for this project emerged from an unexpected place – the ashes of a failed first idea. Sometimes the best opportunities arise from setbacks. This project presented the perfect balance: it satisfied the requirements for my senior project while filling critical knowledge gaps in my development as a Data Engineer. It allowed me to dive deep into cloud migration tools, understand the business considerations that drive cloud adoption decisions, and gain hands-on experience that textbooks simply can’t provide. The project became not just an academic requirement but a stepping stone in my professional journey.\n\n\nBackground Context\nBefore diving into the technical details, let’s understand the foundation of this project. I chose Microsoft’s AdventureWorks, a practice database that simulates a fictional company specializing in bikes and accessories. This was an ideal starting point because it comes with a pre-built database backup (.bak file) that can be easily restored – allowing me to focus on the migration process rather than database creation.\nFor the technical implementation, I selected tools from the Microsoft Azure ecosystem, which offers seamless integration with existing Microsoft products. The technology stack includes:\n- Microsoft SQL Server (source database)\n- Microsoft Azure (cloud platform)\n- Azure Data Factory (data integration service)\n- Azure Data Lake Storage (scalable data storage)\n- Azure Databricks (analytics platform)\nThis combination of tools represents a typical migration path for companies moving their Microsoft-based data systems to the cloud.\n\n\nThe Migration Process\n\nStep 1: Setting Up the On-Premises Environment\nFirst, I had to simulate a typical on-premises setup. I installed SQL Server on my local machine and restored the AdventureWorks lightweight database. This represented the legacy system that many companies still use today – a traditional relational database sitting on a physical server somewhere in the office.\nThe database itself contained 10 tables spanning sales, production, purchasing, and human resources – a complex but realistic dataset that would test my migration skills.\n\n\nStep 2: Designing the Cloud Architecture\nBefore jumping into the migration, I needed a solid plan. I sketched out a modern data architecture with these components: Azure Data Factory for orchestration and data movement Azure Data Lake Storage Gen2 for a temporary storage location Azure Databricks for transformation and analysis into managed tables.\nThis architecture follows the modern data lakehouse pattern, combining the flexibility of data lakes with the reliability and performance of data warehouses.\n\n\nStep 3: Extracting The Data\nUsing Azure Data Factory, I created pipelines to extract data from the on-premises SQL Server and load it into Azure Data Lake Storage. This process involved:\n1. Setting up a self-hosted integration runtime to connect to my local SQL Server\n2. Creating linked services to connect to both source and destination\n3. Developing extraction pipelines for ingesting the data\nOne of the key challenges was encountering network errors. Thankfully with the integration runtime that Azure Data Factory has, that solved those issues.\n\n\nStep 4: Loading and Transforming the Data\nWith the raw data in Azure Data Lake Storage, I used Azure Databricks to transform it into a more analysis-friendly format. This involved:\n\nUsing the medallion architecture (bronze, silver, gold)\nApplying transformations to the data\nCreating aggregations for common analytics needs\nImplementing data quality checks\n\nI used Delta Live Tables for this to learn that method. That introduced its challenges and demanded that I write my code in a specific way, using SWE best practices to accomplish it.\n\n\n\nChallenges and Solutions\n\nLimited Budget\nAs a student, I had limited Azure credits. I could only do so much before running out of resources. So I carefully planned my architecture to minimize costs, using Azure’s free tier where possible and shutting down resources when not in use.\n\n\nSkills\nI needed to learn and even re-learn some tools I had previously worked with. There is so much to the migration, it takes planning and effort to accomplish.\n\n\nNetwork Issues\nIdeally, I would have loved to just pull from my on-prem SQL Server directly into Databricks, reducing the time and tools I needed to learn. But I realized that I was getting now where with the networks from my laptop to the cloud and quickly decided it wasn’t worth the fight for this project.\n\n\n\nLessons Learned\nThis project taught me several valuable lessons: - Cloud migration is never just “lift and shift” – it requires rethinking how data is structured and processed - Data quality issues become extremely apparent during migration - Security and governance need to be baked in from the start, not added later - Automation is essential for repeatable, reliable processes - Documentation is crucial, especially for complex data pipelines\n\n\nConclusion\nData engineering might not be the most visible part of the data world, but it’s the foundation that everything else rests upon. This project gave me deep appreciation for the complexities involved in moving data infrastructure to the cloud.\nWhether you’re a fellow student, a professional looking to transition to data engineering, or someone just curious about what happens behind the scenes, I hope this project has given you insight into this fascinating field.\nRemember, before any data analysis can begin, someone has to make sure the data is available, clean, and ready to use. That’s what data engineers do, and I’m proud to be joining their ranks."
  },
  {
    "objectID": "posts/project overview/index.html#project-architecture",
    "href": "posts/project overview/index.html#project-architecture",
    "title": "Project Overview",
    "section": "",
    "text": "Our solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface"
  },
  {
    "objectID": "posts/project overview/index.html#data-flow-and-processing",
    "href": "posts/project overview/index.html#data-flow-and-processing",
    "title": "Project Overview",
    "section": "",
    "text": "The end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information"
  },
  {
    "objectID": "posts/project overview/index.html#business-benefits",
    "href": "posts/project overview/index.html#business-benefits",
    "title": "Project Overview",
    "section": "",
    "text": "This migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models"
  },
  {
    "objectID": "posts/project overview/index.html#technical-implementation-highlights",
    "href": "posts/project overview/index.html#technical-implementation-highlights",
    "title": "Project Overview",
    "section": "",
    "text": "Our implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance"
  },
  {
    "objectID": "posts/project overview/index.html#conclusion",
    "href": "posts/project overview/index.html#conclusion",
    "title": "Project Overview",
    "section": "",
    "text": "The successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/Why Cloud/index.html",
    "href": "posts/Why Cloud/index.html",
    "title": "Why Migrate to the Cloud?",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\nIntroduction\nCloud computing is no longer a foreign concept - we interact with it daily through services like Google Drive, Dropbox, and Office 365. These familiar tools represent just the surface of what cloud technology offers to businesses and organizations today.\n\n\n\nWhat is Cloud Computing?\nCloud computing delivers technology services—including computing power, storage, databases, and networking—over the internet with a flexible pay-as-you-go pricing model. This approach fundamentally changes how organizations acquire and manage their IT resources.\n\n\nStrategic Advantages of Cloud Migration\n\nOperational Excellence\n\nScalability: Dynamically adjust resources to match demand without capital expenditure on hardware\n\nSpeed: Deploy new applications and services in minutes rather than weeks or months\n\nReliability: Distribute data across multiple geographic regions, eliminating single points of failure\n\nSecurity: Leverage enterprise-grade security measures and compliance certifications from major providers\n\n\n\nBusiness Impact\n\nCost Efficiency: Convert capital expenses to operational expenses with predictable pricing models\n\nInnovation Acceleration: Access cutting-edge technologies without significant upfront investment\n\nGlobal Reach: Deploy resources closer to end-users worldwide with minimal effort\n\nFocus on Core Business: Reduce time spent managing infrastructure to concentrate on strategic initiatives\n\n\n\n\nService Models Explained\nCloud computing offers multiple service models that provide different levels of control and responsibility:\n\n\n\n\n\n\n\n\n\nModel\nTraditional Analogy\nWhat You Manage\nWhat Provider Manages\n\n\n\n\nOn-Premises\nOwning a car\nEverything (hardware, software, maintenance)\nNothing\n\n\nIaaS (Infrastructure as a Service)\nRenting a car\nOS, applications, data\nHardware, networking\n\n\nPaaS (Platform as a Service)\nUsing rideshare (Uber)\nApplications, data\nHardware, OS, middleware\n\n\nSaaS (Software as a Service)\nTaking public transit\nData and access\nEverything else\n\n\n\n\n\nImplementation Roadmap\n\n\n1. Select a Cloud Deployment Model\n\nPublic Cloud: Resources owned and operated by third-party providers\n\nPrivate Cloud: Dedicated resources for a single organization\n\nHybrid Cloud: Combination of public and private clouds with orchestration between them\n\nMulti-Cloud: Strategic use of multiple cloud providers for different services\n\n\n\n2. Develop Necessary Expertise\n\nRoles to Consider: Cloud architects, cloud engineers, DevOps specialists, security experts\n\nSkills to Acquire: Cloud provider certifications, infrastructure as code, containerization, orchestration\n\n\n\n3. Choose Appropriate Provider(s)\n\nMajor Players: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)\n\nSelection Criteria: Service offerings, pricing models, geographic presence, integration capabilities\n\n\n\n4. Create a Comprehensive Migration Strategy\n\nAssessment: Inventory current applications and infrastructure\n\nPlanning: Determine migration approach (rehost, refactor, rearchitect, rebuild)\n\nExecution: Implement the migration with minimal disruption\n\nOptimization: Continuously improve cloud resources and configurations\n\n\n\n\n\nConclusion\nCloud migration has evolved from a technological trend to a business imperative. The combination of financial benefits, operational improvements, and innovation potential makes cloud adoption essential for organizations seeking to remain competitive in the digital landscape.\nBy strategically implementing cloud computing services, companies can enhance agility, improve efficiency, and accelerate innovation while simultaneously reducing technical debt and operational costs. This transformation allows organizations to redirect focus toward their core business objectives and customer needs rather than infrastructure management.\nThe journey to the cloud requires careful planning and execution, but the long-term benefits far outweigh the initial investment in time and resources. As demonstrated in my project, a well-designed migration strategy provides a clear pathway to realizing the full potential of cloud computing."
  },
  {
    "objectID": "posts/why_cloud/index.html",
    "href": "posts/why_cloud/index.html",
    "title": "Why Migrate to the Cloud?",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\nIntroduction\nCloud computing is no longer a foreign concept - we interact with it daily through services like Google Drive, Dropbox, and Office 365! These familiar tools represent just the surface of what cloud technology offers to businesses and organizations today.\n\n\n\nWhat is Cloud Computing?\nCloud computing delivers technology services—including computing power, storage, databases, and networking—over the internet with a flexible pay-as-you-go pricing model. This approach fundamentally changes how organizations acquire and manage their IT resources.\n\n\nStrategic Advantages of Cloud Migration\n\nOperational Excellence\nMoving to the cloud has many benefits. This is in part to the service models you have access to. But more on that in a moment. Here are some of the operating advantages available to you:\n\nScalability: Dynamically adjust resources to match demand without capital expenditure on hardware\n\nSpeed: Deploy new applications and services in minutes rather than weeks or months\n\nReliability: Distribute data across multiple geographic regions, eliminating single points of failure\n\nSecurity: Leverage enterprise-grade security measures and compliance certifications from major providers\n\n\n\nBusiness Impact\nSo, how do the advantages listed above impact you? Here are just a few ways:\n\nCost Efficiency: Convert capital expenses to operational expenses with predictable pricing models\n\nInnovation Acceleration: Access cutting-edge technologies without significant upfront investment\n\nGlobal Reach: Deploy resources closer to end-users worldwide with minimal effort\n\nFocus on Core Business: Reduce time spent managing infrastructure to concentrate on strategic initiatives\n\n\n\n\nService Models Explained\nCloud computing offers multiple service models that provide different levels of control and responsibility:\n\n\n\n\n\n\n\n\n\nModel\nTraditional Analogy\nWhat You Manage\nWhat Provider Manages\n\n\n\n\nOn-Premises\nOwning a car\nEverything (hardware, software, maintenance)\nNothing\n\n\nIaaS (Infrastructure as a Service)\nRenting a car\nOS, applications, data\nHardware, networking\n\n\nPaaS (Platform as a Service)\nUsing rideshare (Uber)\nApplications, data\nHardware, OS, middleware\n\n\nSaaS (Software as a Service)\nTaking public transit\nData and access\nEverything else\n\n\n\nCan you see how some of those strategic advantages might come into play? While there are times you will want the flexibility to adjust everything manually, like On-Premise models, having the availability of IaaS, PaaS, and SaaS models as you move into the cloud can be a game changer!\n\n\nImplementation Roadmap\n\n\n1. Select a Cloud Deployment Model\nAs you being, you’ll need to take into account what is right for your situation. Which scenario below makes the most sense for you?\n\nPublic Cloud: Resources owned and operated by third-party providers\n\nPrivate Cloud: Dedicated resources for a single organization\n\nHybrid Cloud: Combination of public and private clouds with orchestration between them\n\nMulti-Cloud: Strategic use of multiple cloud providers for different services\n\n\n\n2. Develop Necessary Expertise\nNow that you know you want to move to the cloud, you’re gonna need the skills or people to make it happen.\n\nRoles to Consider: Cloud architects, cloud engineers, DevOps specialists, security experts\n\nSkills to Acquire: Cloud provider certifications, infrastructure as code, containerization, orchestration\n\n\n\n3. Choose Appropriate Provider(s)\nThe providers will determine alot. While they all offer similar things, there are some differences to them. Consider first what will work for you, then see which provider fits best.\n\nMajor Players: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)\n\nSelection Criteria: Service offerings, pricing models, geographic presence, integration capabilities\n\n\n\n4. Create a Comprehensive Migration Strategy\nIt’s time to put the planning into action. Ask questions like, “What is our current situation?” and “What would be the best way to go about this?” to get you on the right track.\n\nAssessment: Inventory current applications and infrastructure\n\nPlanning: Determine migration approach (rehost, refactor, rearchitect, rebuild)\n\nExecution: Implement the migration with minimal disruption\n\nOptimization: Continuously improve cloud resources and configurations\n\n\n\n\n\nConclusion\nCloud migration has evolved from a technological trend to a business imperative. The combination of financial benefits, operational improvements, and innovation potential makes cloud adoption essential for organizations seeking to remain competitive in the digital landscape. This transformation allows organizations to redirect focus toward their core business objectives and customer needs rather than infrastructure management.\nThe journey to the cloud requires careful planning and execution, but the long-term benefits far outweigh the initial investment in time and resources."
  },
  {
    "objectID": "posts/my_story/post.html",
    "href": "posts/my_story/post.html",
    "title": "Is A Career in Data Right For You?",
    "section": "",
    "text": "Picture this. You work at a massive company making buko bucks and you’re the guy everyone comes to when they need to make an important decision. The most recent question you’ve been asked is if the company should invest a ton of money into a new product or not. What would you do?\nHow about if you were working at a pizza place instead and you and your co-workers notice that at specific times of the day, days of the week, or holidays, that it seems to get a whole lot busier and there aren’t enough employees to handle the volume. What would you do?\nThis is where data becomes your best friend.\nIn the first example, you might start first by researching. What is this product? What’s the price? Who sells it? Are they reputable? Maybe you find yourself asking questions that are more company facing. Do you have capacity for it? Would it help your companies trajectory? Maybe you find another person to bounce some of those questions off of that could help you gain more insights.\nIn the second example, maybe you start counting how many customers came in at certain times, days, and seemingly outlier events. You notice that not many people come in for pizza in the morning, but they sure do on friday evening. You bring this to your boss, present and argument for more staff and show them what you’ve compilled. That would be a pretty convincing argument.\nIn both of these cases, data was gathered. Sometimes its just intentional information sought after and other times its gathered, visualized, and drives actionable insights. Either way, its incredible useful and a lot more insightful than the alternative."
  },
  {
    "objectID": "posts/project_overview/index.html",
    "href": "posts/project_overview/index.html",
    "title": "Behind the Scenes: My Senior Project on Cloud Data Migration",
    "section": "",
    "text": "Introduction\nDid you know that behind every analysis, dashboard, and report lies an intricate process of data wrangling? Before any insights can be drawn, data must first be collected, cleaned, and formatted appropriately. In the industry, this crucial work falls to Data Engineers – the unsung heroes who extract data from various sources, transform it through complex processes, and deliver it in a usable state to Data Scientists and Analysts. Without these engineers laying the groundwork, data analysis simply couldn’t happen.\nYou might be wondering, “What does this data engineering work actually look like in practice?” That’s exactly what this blog post will show you.\nMy senior project tackles a scenario that Data Engineers commonly face: migrating an existing data infrastructure to the cloud. If terms like “data migration” and “cloud infrastructure” sound like technical jargon, don’t worry – I’ve written another article called “Why Migrate to the Cloud?” that explains these concepts in more detail.\nThrough this project, I hope to illuminate a vital but often overlooked aspect of the data world and highlight the essential role that Data Engineers play in enabling modern analytics.\n\n\nInspiration\nThe inspiration for this project emerged from an unexpected place – the ashes of a failed first idea. Sometimes the best opportunities arise from setbacks. This project presented the perfect balance: it satisfied the requirements for my senior project while filling critical knowledge gaps in my development as a Data Engineer. It allowed me to dive deep into cloud migration tools, understand the business considerations that drive cloud adoption decisions, and gain hands-on experience that textbooks simply can’t provide. The project became not just an academic requirement but a stepping stone in my professional journey.\n\n\nBackground Context\nBefore diving into the technical details, let’s understand the foundation of this project. I chose Microsoft’s AdventureWorks, a practice database that simulates a fictional company specializing in bikes and accessories. This was an ideal starting point because it comes with a pre-built database backup (.bak file) that can be easily restored – allowing me to focus on the migration process rather than database creation.\nFor the technical implementation, I selected tools from the Microsoft Azure ecosystem, which offers seamless integration with existing Microsoft products. The technology stack includes:\n\nMicrosoft SQL Server (source database)\n\nMicrosoft Azure (cloud platform)\n\nAzure Data Factory (data integration service)\n\nAzure Data Lake Storage (scalable data storage)\n\nAzure Databricks (analytics platform)\n\nThis combination of tools represents a typical migration path for companies moving their Microsoft-based data systems to the cloud.\n\n\n\nThe Migration Process\n\nStep 1: Setting Up the On-Premises Environment\nFirst, I had to simulate a typical on-premises setup. I installed SQL Server on my local machine and restored the AdventureWorks lightweight database. This represented the legacy system that many companies still use today – a traditional relational database sitting on a physical server somewhere in the office.\nThe database itself contained 10 tables spanning sales, production, purchasing, and human resources – a complex but realistic dataset that would test my migration skills.\n\n\nStep 2: Designing the Cloud Architecture\nBefore jumping into the migration, I needed a solid plan. I sketched out a modern data architecture with these components: Azure Data Factory for orchestration and data movement Azure Data Lake Storage Gen2 for a temporary storage location Azure Databricks for transformation and analysis into managed tables.\nThis architecture follows the modern data lakehouse pattern, combining the flexibility of data lakes with the reliability and performance of data warehouses.\n\n\nStep 3: Extracting The Data\nUsing Azure Data Factory, I created pipelines to extract data from the on-premises SQL Server and load it into Azure Data Lake Storage. This process involved:\n\nSetting up a self-hosted integration runtime to connect to my local SQL Server\n\nCreating linked services to connect to both source and destination\n\nDeveloping extraction pipelines for ingesting the data\n\nOne of the key challenges was encountering network errors. Thankfully with the integration runtime that Azure Data Factory has, that solved those issues.\n\n\nStep 4: Loading and Transforming the Data\nWith the raw data in Azure Data Lake Storage, I used Azure Databricks to transform it into a more analysis-friendly format. This involved:\n\nUsing the medallion architecture (bronze, silver, gold)\nApplying transformations to the data\nCreating aggregations for common analytics needs\nImplementing data quality checks\n\nI used Delta Live Tables for this to learn that method. That introduced its challenges and demanded that I write my code in a specific way, using SWE best practices to accomplish it.\n\n\n\nChallenges and Solutions\n\nLimited Budget\nAs a student, I had limited Azure credits. I could only do so much before running out of resources. So I carefully planned my architecture to minimize costs, using Azure’s free tier where possible and shutting down resources when not in use.\n\n\nSkills\nI needed to learn and even re-learn some tools I had previously worked with. There is so much to the migration, it takes planning and effort to accomplish.\n\n\nNetwork Issues\nIdeally, I would have loved to just pull from my on-prem SQL Server directly into Databricks, reducing the time and tools I needed to learn. But I realized that I was getting now where with the networks from my laptop to the cloud and quickly decided it wasn’t worth the fight for this project.\n\n\n\nLessons Learned\nThis project taught me several valuable lessons: - Cloud migration is never just “lift and shift” – it requires rethinking how data is structured and processed - Data quality issues become extremely apparent during migration - Security and governance need to be baked in from the start, not added later - Automation is essential for repeatable, reliable processes - Documentation is crucial, especially for complex data pipelines\n\n\nConclusion\nData engineering might not be the most visible part of the data world, but it’s the foundation that everything else rests upon. This project gave me deep appreciation for the complexities involved in moving data infrastructure to the cloud.\nWhether you’re a fellow student, a professional looking to transition to data engineering, or someone just curious about what happens behind the scenes, I hope this project has given you insight into this fascinating field.\nRemember, before any data analysis can begin, someone has to make sure the data is available, clean, and ready to use. That’s what data engineers do, and I’m proud to be joining their ranks."
  },
  {
    "objectID": "posts/my_story/post.html#understanding-the-data-landscape",
    "href": "posts/my_story/post.html#understanding-the-data-landscape",
    "title": "Is A Career in Data Right For You?",
    "section": "Understanding the Data Landscape",
    "text": "Understanding the Data Landscape\nThe world of data is vast and diverse, encompassing several specialized roles:\n\nData Analysts interpret data to solve business problems\nData Scientists build complex models and algorithms\nData Engineers design and maintain data infrastructure\nMachine Learning Engineers develop AI and predictive systems\nBusiness Intelligence Analysts create visualizations and reports"
  },
  {
    "objectID": "posts/my_story/post.html#key-skills-for-success-in-data",
    "href": "posts/my_story/post.html#key-skills-for-success-in-data",
    "title": "Is A Career in Data Right For You?",
    "section": "Key Skills for Success in Data",
    "text": "Key Skills for Success in Data\n\nTechnical Skills\n\nProgramming languages (Python, R, SQL)\nStatistical analysis\nMachine learning techniques\nData visualization tools\nDatabase management\n\n\n\nSoft Skills\n\nCritical thinking\nProblem-solving\nCommunication\nCuriosity\nAttention to detail"
  },
  {
    "objectID": "posts/my_story/post.html#signs-you-might-be-a-great-fit-for-a-data-career",
    "href": "posts/my_story/post.html#signs-you-might-be-a-great-fit-for-a-data-career",
    "title": "Is A Career in Data Right For You?",
    "section": "Signs You Might Be a Great Fit for a Data Career",
    "text": "Signs You Might Be a Great Fit for a Data Career\n\n1. You Love Solving Puzzles\nData professionals are essentially professional puzzle solvers. If you enjoy breaking down complex problems, finding patterns, and uncovering insights, this career could be your calling.\n\n\n2. You’re Comfortable with Continuous Learning\nTechnology and data tools evolve rapidly. A successful data professional must be committed to lifelong learning and staying current with emerging technologies.\n\n\n3. You Enjoy Working with Numbers\nWhile not every data role requires advanced mathematical skills, comfort with numbers, statistics, and analytical thinking is crucial.\n\n\n4. You’re Curious by Nature\nThe best data professionals are those who ask “why” and “how” constantly, always seeking to understand the story behind the numbers."
  },
  {
    "objectID": "posts/my_story/post.html#potential-challenges-to-consider",
    "href": "posts/my_story/post.html#potential-challenges-to-consider",
    "title": "Is A Career in Data Right For You?",
    "section": "Potential Challenges to Consider",
    "text": "Potential Challenges to Consider\n\n1. Technical Complexity\nLearning programming, statistics, and complex tools can be challenging and requires dedication.\n\n\n2. Rapid Technological Change\nWhat’s cutting-edge today might be obsolete tomorrow, requiring constant upskilling.\n\n\n3. Data Privacy and Ethical Considerations\nWorking with data comes with significant responsibilities around privacy and ethical use of information."
  },
  {
    "objectID": "posts/my_story/post.html#how-to-get-started",
    "href": "posts/my_story/post.html#how-to-get-started",
    "title": "Is A Career in Data Right For You?",
    "section": "How to Get Started",
    "text": "How to Get Started\n\nEducation: Consider degrees in computer science, statistics, mathematics, or related fields\nOnline Courses: Platforms like Coursera, edX, and Udacity offer excellent data science programs\nCertifications: Look into AWS, Google, and Microsoft data certifications\nBuild a Portfolio: Create projects on GitHub to showcase your skills\nNetwork: Join data science communities and attend industry conferences"
  },
  {
    "objectID": "posts/my_story/post.html#salary-and-job-market",
    "href": "posts/my_story/post.html#salary-and-job-market",
    "title": "Is A Career in Data Right For You?",
    "section": "Salary and Job Market",
    "text": "Salary and Job Market\nThe data field offers competitive salaries and robust job growth. According to recent industry reports:\n\nAverage data scientist salary: $95,000 - $130,000\nProjected job growth: 36% (much faster than average)\nOpportunities across industries: technology, healthcare, finance, marketing, and more"
  },
  {
    "objectID": "posts/my_story/post.html#final-thoughts",
    "href": "posts/my_story/post.html#final-thoughts",
    "title": "Is A Career in Data Right For You?",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nA career in data isn’t just about numbers—it’s about telling stories, solving problems, and driving meaningful change. If you’re passionate about transforming raw information into actionable insights, this might be the perfect career path for you.\nRemember, there’s no one-size-fits-all approach. Explore, experiment, and find the data niche that resonates with your interests and strengths.\nAre you ready to embark on your data journey?"
  }
]