[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "From Databricks to Power BI",
    "section": "",
    "text": "Everything you need to know about importing data from Databricks to Power BI.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/first_blog_post/post.html",
    "href": "posts/first_blog_post/post.html",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Mastering Your Workflow: A Data Analyst’s Roadmap to Success\nIn the world of data analysis, having a solid workflow isn’t just a nice-to-have—it’s your secret weapon for delivering exceptional results. A well-crafted workflow helps you:\n\nSet crystal-clear expectations for your project\nDefine and measure key outcomes with precision\nBoost your accuracy and efficiency\nConsistently deliver high-quality, insightful work\n\nEvery project is unique, and so is every workflow. But one thing remains constant: having a structured approach is always better than flying blind.\n\n\n1. Understanding the Business Case\nBefore diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n2. Measurement Planning\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n3. Collect and Prep Data\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n4. Understand the Data\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity -Composition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n5. Analyze and Visualize\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n6. Develop Data-driven Insights\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n7. Measure, Test, Optimize\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities\n\n\nRemember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value. By embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?\n\n\n\n\n\nA breakdown of each role, their responsibilities, and important skills.\n\n\n\n\n\nMar 5, 2025\n\n\nSarah Dorny\n\n\n\n\n\n\n\n\n\n\n\n\nProject Overview\n\n\n\n\n\n\nProject\n\n\n\nA Brief Overview of My Senior Project: On-Prem SQL Server to the Cloud.\n\n\n\n\n\nFeb 10, 2025\n\n\nSarah Dorny\n\n\n\n\n\n\n\n\n\n\n\n\nThe Importance of a Workflow\n\n\n\n\n\n\nSoft Skills\n\n\nData Analytics\n\n\n\nWithout a process, you are flying blind.\n\n\n\n\n\nJan 25, 2022\n\n\nSarah Dorny\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Sarah Dorny.",
    "section": "",
    "text": "Detail-oriented Data Scientist with over 2 years of industry experience in business intelligence and project management. Skilled in data extraction, modeling, and visualization to deliver actionable insights. Microsoft Power BI certified (PL-300)."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Why Teamwork Matters",
    "section": "",
    "text": "“If everyone is moving forward together, then success takes care of itself.” - Henry Ford"
  },
  {
    "objectID": "posts/difference/index.html",
    "href": "posts/difference/index.html",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "In today’s data-driven world, organizations rely on specialized professionals to extract value from their information assets. Three key roles have emerged as pillars in the data ecosystem: Data Engineers, Data Scientists, and Data Analysts. While these positions share a common foundation in data, they serve distinct functions with different skill sets, tools, and responsibilities.\n\n\n\n\n\nData Engineers are the architects who design, build, and maintain the infrastructure that allows data to flow throughout an organization. They create the pipelines that transport data from various sources to storage systems where it can be accessed and analyzed.\n\n\n\n\nProgramming: Proficiency in languages like Python, Java, or Scala\nDatabase Systems: Expert knowledge of SQL and NoSQL databases\nBig Data Technologies: Experience with tools like Hadoop, Spark, and Kafka\nETL Processes: Building robust Extract, Transform, Load pipelines\nCloud Platforms: Working with AWS, Azure, or Google Cloud services\n\n\n\n\nA typical day for a Data Engineer might involve:\n- Designing database schemas\n- Optimizing data flows for performance\n- Implementing data quality checks\n- Setting up data warehousing solutions\n- Creating APIs for data access\n\n\n\nMost Data Engineers come from software engineering or computer science backgrounds and have strong technical foundations in system architecture.\n\n\n\n\n\n\n\nData Scientists combine advanced statistical methods, machine learning, and domain expertise to extract deeper insights, make predictions, and solve complex problems with data.\n\n\n\n\nAdvanced Statistics: Hypothesis testing, experimental design\nMachine Learning: Building predictive models\nProgramming: Strong coding skills in Python or R\nData Storytelling: Communicating complex findings\nDomain Expertise: Understanding business context\n\n\n\n\nA typical day for a Data Scientist might involve: - Developing predictive algorithms - Training and evaluating ML models - Conducting exploratory data analysis - Testing hypotheses with statistical methods - Researching new analytical approaches\n\n\n\nData Scientists typically have advanced degrees in quantitative fields like statistics, mathematics, computer science, or specialized data science programs.\n\n\n\n\n\n\n\nData Analysts focus on examining existing data to answer specific business questions and provide actionable insights. They translate data into information that guides decision-making.\n\n\n\n\nStatistical Analysis: Understanding descriptive statistics\nData Visualization: Creating charts and dashboards\nSQL: Writing queries to extract and manipulate data\nBusiness Intelligence Tools: Proficiency with Tableau, Power BI, or Looker\nSpreadsheet Mastery: Advanced Excel or Google Sheets techniques\n\n\n\n\nA typical day for a Data Analyst might involve: - Creating performance reports - Building interactive dashboards - Conducting A/B test analysis - Identifying trends in customer behavior - Presenting findings to stakeholders\n\n\n\nData Analysts often come from diverse backgrounds including business, economics, mathematics, or even self-taught paths with strong analytical mindsets.\n\n\n\n\nIn an efficient data organization, these three roles form a symbiotic relationship:\n\nData Engineers build the infrastructure and pipelines that collect and organize raw data\nData Scientists leverage the same infrastructure to build models that predict future outcomes\nData Analysts use this data to explain current business performance and trends\n\n\n\n\nIt’s worth noting that career paths in data can be fluid:\n\nSome Data Analysts develop programming and ML skills to become Data Scientists\nSome Data Scientists move into Data Engineering to better understand the full data stack\nOthers specialize further within their domain, becoming experts in specific industries or techniques\n\n\n\n\n\n\n\n\n\n\n\n\n\nRole\nEntry-Level\nMid-Career\nSenior\n\n\n\n\nData Engineer\n$85,000-$110,000\n$110,000-$140,000\n$140,000-$180,000+\n\n\nData Analyst\n$65,000-$85,000\n$85,000-$115,000\n$115,000-$145,000+\n\n\nData Scientist\n$90,000-$120,000\n$120,000-$150,000\n$150,000-$200,000+\n\n\n\n*Note: Salaries vary significantly based on location, industry, company size, and individual experience.\n\n\n\nWhile all three roles work with data, they serve complementary functions in the data value chain:\n\nData Engineers focus on the infrastructure that makes data available\nData Scientists focus on models that predict what will happen next\nData Analysts focus on the insights that explain what’s happening now\n\nUnderstanding these distinctions can help organizations build balanced data teams and help individuals chart their career paths in this rapidly evolving field."
  },
  {
    "objectID": "posts/difference/index.html#data-engineers-building-the-foundation",
    "href": "posts/difference/index.html#data-engineers-building-the-foundation",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Engineers are the architects who design, build, and maintain the infrastructure that allows data to flow throughout an organization. They create the pipelines that transport data from various sources to storage systems where it can be accessed and analyzed.\n\n\n\n\nProgramming: Proficiency in languages like Python, Java, or Scala\nDatabase Systems: Expert knowledge of SQL and NoSQL databases\nBig Data Technologies: Experience with tools like Hadoop, Spark, and Kafka\nETL Processes: Building robust Extract, Transform, Load pipelines\nCloud Platforms: Working with AWS, Azure, or Google Cloud services\n\n\n\n\nA typical day for a Data Engineer might involve:\n- Designing database schemas\n- Optimizing data flows for performance\n- Implementing data quality checks\n- Setting up data warehousing solutions\n- Creating APIs for data access\n\n\n\nMost Data Engineers come from software engineering or computer science backgrounds and have strong technical foundations in system architecture."
  },
  {
    "objectID": "posts/difference/index.html#data-analysts-interpreting-the-present",
    "href": "posts/difference/index.html#data-analysts-interpreting-the-present",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Analysts focus on examining existing data to answer specific business questions and provide actionable insights. They translate data into information that guides decision-making.\n\n\n\n\nStatistical Analysis: Understanding descriptive statistics\nData Visualization: Creating charts and dashboards\nSQL: Writing queries to extract and manipulate data\nBusiness Intelligence Tools: Proficiency with Tableau, Power BI, or Looker\nSpreadsheet Mastery: Advanced Excel or Google Sheets techniques\n\n\n\n\nA typical day for a Data Analyst might involve: - Creating performance reports - Building interactive dashboards - Conducting A/B test analysis - Identifying trends in customer behavior - Presenting findings to stakeholders\n\n\n\nData Analysts often come from diverse backgrounds including business, economics, mathematics, or even self-taught paths with strong analytical mindsets."
  },
  {
    "objectID": "posts/difference/index.html#data-scientists-predicting-the-future",
    "href": "posts/difference/index.html#data-scientists-predicting-the-future",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Scientists combine advanced statistical methods, machine learning, and domain expertise to extract deeper insights, make predictions, and solve complex problems with data.\n\n\n\n\nAdvanced Statistics: Hypothesis testing, experimental design\nMachine Learning: Building predictive models\nProgramming: Strong coding skills in Python or R\nData Storytelling: Communicating complex findings\nDomain Expertise: Understanding business context\n\n\n\n\nA typical day for a Data Scientist might involve: - Developing predictive algorithms - Training and evaluating ML models - Conducting exploratory data analysis - Testing hypotheses with statistical methods - Researching new analytical approaches\n\n\n\nData Scientists typically have advanced degrees in quantitative fields like statistics, mathematics, computer science, or specialized data science programs."
  },
  {
    "objectID": "posts/difference/index.html#how-these-roles-interact",
    "href": "posts/difference/index.html#how-these-roles-interact",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "In an efficient data organization, these three roles form a symbiotic relationship:\n\nData Engineers build the infrastructure and pipelines that collect and organize raw data\nData Scientists leverage the same infrastructure to build models that predict future outcomes\nData Analysts use this data to explain current business performance and trends"
  },
  {
    "objectID": "posts/difference/index.html#career-progression",
    "href": "posts/difference/index.html#career-progression",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "It’s worth noting that career paths in data can be fluid:\n\nSome Data Analysts develop programming and ML skills to become Data Scientists\nSome Data Scientists move into Data Engineering to better understand the full data stack\nOthers specialize further within their domain, becoming experts in specific industries or techniques"
  },
  {
    "objectID": "posts/difference/index.html#salary-comparisons-as-of-2024",
    "href": "posts/difference/index.html#salary-comparisons-as-of-2024",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Role\nEntry-Level\nMid-Career\nSenior\n\n\n\n\nData Engineer\n$85,000-$110,000\n$110,000-$140,000\n$140,000-$180,000+\n\n\nData Analyst\n$65,000-$85,000\n$85,000-$115,000\n$115,000-$145,000+\n\n\nData Scientist\n$90,000-$120,000\n$120,000-$150,000\n$150,000-$200,000+\n\n\n\n*Note: Salaries vary significantly based on location, industry, company size, and individual experience."
  },
  {
    "objectID": "posts/difference/index.html#conclusion",
    "href": "posts/difference/index.html#conclusion",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "While all three roles work with data, they serve complementary functions in the data value chain:\n\nData Engineers focus on the infrastructure that makes data available\nData Scientists focus on models that predict what will happen next\nData Analysts focus on the insights that explain what’s happening now\n\nUnderstanding these distinctions can help organizations build balanced data teams and help individuals chart their career paths in this rapidly evolving field."
  },
  {
    "objectID": "posts/teamwork/index.html",
    "href": "posts/teamwork/index.html",
    "title": "Project Overview",
    "section": "",
    "text": "Our organization recently completed a strategic data platform migration from an on-premise SQL Server environment to a modern cloud-based analytics ecosystem centered on Azure Databricks. This transformation has enabled us to leverage advanced data processing capabilities, implement a robust data lakehouse architecture, and deliver more powerful business insights through enhanced visualization.\n\n\n\nOur solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface\n\n\n\n\nThe end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information\n\n\n\n\n\nThis migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models\n\n\n\n\n\nOur implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance\n\n\n\n\nThe successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/workflow/post.html",
    "href": "posts/workflow/post.html",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "In the world of data analysis, having a solid workflow isn’t just a nice-to-have—it’s your secret weapon for delivering exceptional results. A well-crafted workflow helps you:\n\nSet crystal-clear expectations for your project\nDefine and measure key outcomes with precision\nBoost your accuracy and efficiency\nConsistently deliver high-quality, insightful work\n\nEvery project is unique, and so is every workflow. But one thing remains constant: having a structured approach is always better than flying blind.\n\n\n\n\n\nBefore diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity\nComposition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities\n\n\n\n\n\nLet’s walk through this workflow with a practical example: analyzing customer churn for an e-commerce company.\n\n\nScenario: The marketing director approaches you concerned about declining customer retention rates.\nKey Questions Addressed:\n- Business goal: Reduce customer churn by 15% within 6 months\n- Stakeholders: Marketing, Sales, and Product teams\n- Motivation: Customer acquisition costs are 5x higher than retention costs\n- Strategic alignment: Company’s annual goal to increase customer lifetime value\n\n\n\nDefined Success Metrics:\n- Primary KPI: 30-day customer churn rate\n- Secondary KPIs: Repeat purchase rate, average time between purchases, cart abandonment rate\n- Data needed: Transaction history, customer demographics, website behavior, support interactions\n\n\n\nActions Taken:\n- Extracted 18 months of transaction data from the CRM\n- Merged with customer profile data from the marketing database\n- Cleaned inconsistent date formats and duplicate customer IDs\n- Created feature for “days since last purchase” and “purchase frequency”\n- Automated weekly data refresh using SQL stored procedures\n\n\n\nKey Discoveries:\n- Data covered 45,000 customers and 120,000 transactions\n- Granularity: Individual transaction level with timestamps\n- Identified seasonal purchasing patterns\n- Recognized that mobile app users had different behavior patterns than website users\n\n\n\nApproach:\n- Created cohort analysis showing retention rates by acquisition channel\n- Developed a churn prediction model using purchase recency, frequency, and monetary value\n- Visualized customer journey maps highlighting dropout points\n- Built an interactive dashboard showing churn trends by customer segment\n\n\n\nKey Findings:\n- 64% of churned customers abandoned their shopping cart in their last session\n- Customers who contacted support were 3x more likely to churn\n- First-time customers who purchased during a major sale had 40% higher churn rates\n- Recommendation: Implement targeted retention campaigns for high-risk segments with personalized offers based on previous purchase history\n\n\n\nResults:\n- A/B tested email re-engagement campaigns with 10% of at-risk customers\n- Most effective campaign reduced churn by 22% in the test group\n- Full implementation projected to save $250,000 annually in customer acquisition costs\n- Continuing to refine customer segmentation based on new behavioral data\n\n\n\n\nRemember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value.\nBy embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change. The e-commerce churn example demonstrates how following a structured workflow transforms raw data into valuable business outcomes—precisely what separates exceptional analysts from the rest."
  },
  {
    "objectID": "posts/workflow/post.html#the-7-step-data-analyst-workflow",
    "href": "posts/workflow/post.html#the-7-step-data-analyst-workflow",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Before diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity\nComposition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities"
  },
  {
    "objectID": "posts/workflow/post.html#real-world-example-e-commerce-customer-churn-analysis",
    "href": "posts/workflow/post.html#real-world-example-e-commerce-customer-churn-analysis",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Let’s walk through this workflow with a practical example: analyzing customer churn for an e-commerce company.\n\n\nScenario: The marketing director approaches you concerned about declining customer retention rates.\nKey Questions Addressed:\n- Business goal: Reduce customer churn by 15% within 6 months\n- Stakeholders: Marketing, Sales, and Product teams\n- Motivation: Customer acquisition costs are 5x higher than retention costs\n- Strategic alignment: Company’s annual goal to increase customer lifetime value\n\n\n\nDefined Success Metrics:\n- Primary KPI: 30-day customer churn rate\n- Secondary KPIs: Repeat purchase rate, average time between purchases, cart abandonment rate\n- Data needed: Transaction history, customer demographics, website behavior, support interactions\n\n\n\nActions Taken:\n- Extracted 18 months of transaction data from the CRM\n- Merged with customer profile data from the marketing database\n- Cleaned inconsistent date formats and duplicate customer IDs\n- Created feature for “days since last purchase” and “purchase frequency”\n- Automated weekly data refresh using SQL stored procedures\n\n\n\nKey Discoveries:\n- Data covered 45,000 customers and 120,000 transactions\n- Granularity: Individual transaction level with timestamps\n- Identified seasonal purchasing patterns\n- Recognized that mobile app users had different behavior patterns than website users\n\n\n\nApproach:\n- Created cohort analysis showing retention rates by acquisition channel\n- Developed a churn prediction model using purchase recency, frequency, and monetary value\n- Visualized customer journey maps highlighting dropout points\n- Built an interactive dashboard showing churn trends by customer segment\n\n\n\nKey Findings:\n- 64% of churned customers abandoned their shopping cart in their last session\n- Customers who contacted support were 3x more likely to churn\n- First-time customers who purchased during a major sale had 40% higher churn rates\n- Recommendation: Implement targeted retention campaigns for high-risk segments with personalized offers based on previous purchase history\n\n\n\nResults:\n- A/B tested email re-engagement campaigns with 10% of at-risk customers\n- Most effective campaign reduced churn by 22% in the test group\n- Full implementation projected to save $250,000 annually in customer acquisition costs\n- Continuing to refine customer segmentation based on new behavioral data"
  },
  {
    "objectID": "posts/workflow/post.html#conclusion",
    "href": "posts/workflow/post.html#conclusion",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Remember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value.\nBy embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change. The e-commerce churn example demonstrates how following a structured workflow transforms raw data into valuable business outcomes—precisely what separates exceptional analysts from the rest."
  },
  {
    "objectID": "posts/teamwork/index.html#executive-summary",
    "href": "posts/teamwork/index.html#executive-summary",
    "title": "Project Overview",
    "section": "",
    "text": "Our organization recently completed a strategic data platform migration from an on-premise SQL Server environment to a modern cloud-based analytics ecosystem centered on Azure Databricks. This transformation has enabled us to leverage advanced data processing capabilities, implement a robust data lakehouse architecture, and deliver more powerful business insights through enhanced visualization."
  },
  {
    "objectID": "posts/teamwork/index.html#project-architecture",
    "href": "posts/teamwork/index.html#project-architecture",
    "title": "Project Overview",
    "section": "",
    "text": "Our solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface"
  },
  {
    "objectID": "posts/teamwork/index.html#data-flow-and-processing",
    "href": "posts/teamwork/index.html#data-flow-and-processing",
    "title": "Project Overview",
    "section": "",
    "text": "The end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information"
  },
  {
    "objectID": "posts/teamwork/index.html#business-benefits",
    "href": "posts/teamwork/index.html#business-benefits",
    "title": "Project Overview",
    "section": "",
    "text": "This migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models"
  },
  {
    "objectID": "posts/teamwork/index.html#technical-implementation-highlights",
    "href": "posts/teamwork/index.html#technical-implementation-highlights",
    "title": "Project Overview",
    "section": "",
    "text": "Our implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance"
  },
  {
    "objectID": "posts/teamwork/index.html#conclusion",
    "href": "posts/teamwork/index.html#conclusion",
    "title": "Project Overview",
    "section": "",
    "text": "The successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  }
]