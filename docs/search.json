[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "From Databricks to Power BI",
    "section": "",
    "text": "Everything you need to know about importing data from Databricks to Power BI.\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/first_blog_post/post.html",
    "href": "posts/first_blog_post/post.html",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Mastering Your Workflow: A Data Analyst’s Roadmap to Success\nIn the world of data analysis, having a solid workflow isn’t just a nice-to-have—it’s your secret weapon for delivering exceptional results. A well-crafted workflow helps you:\n\nSet crystal-clear expectations for your project\nDefine and measure key outcomes with precision\nBoost your accuracy and efficiency\nConsistently deliver high-quality, insightful work\n\nEvery project is unique, and so is every workflow. But one thing remains constant: having a structured approach is always better than flying blind.\n\n\n1. Understanding the Business Case\nBefore diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n2. Measurement Planning\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n3. Collect and Prep Data\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n4. Understand the Data\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity -Composition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n5. Analyze and Visualize\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n6. Develop Data-driven Insights\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n7. Measure, Test, Optimize\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities\n\n\nRemember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value. By embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\n\n\n\n\nA Deep Dive into Why Everyone Is Moving to the Cloud and Why You Should Consider It Too.\n\n\n\n\n\nMar 5, 2025\n\n\nSarah Dorny\n\n\n\n\n\n\n\n\n\n\n\n\nProject Overview\n\n\n\n\n\n\nProject\n\n\n\nA brief overview of my senior project: on-prem SQL server to the cloud.\n\n\n\n\n\nFeb 10, 2025\n\n\nSarah Dorny\n\n\n\n\n\n\n\n\n\n\n\n\nThe Importance of a Workflow\n\n\n\n\n\n\nSoft Skills\n\n\nData Analytics\n\n\n\nWithout a process, you are flying blind.\n\n\n\n\n\nJan 25, 2022\n\n\nSarah Dorny\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Sarah Dorny.",
    "section": "",
    "text": "Detail-oriented Data Scientist with over 2 years of industry experience in business intelligence and project management. Skilled in data extraction, modeling, and visualization to deliver actionable insights. Microsoft Power BI certified (PL-300)."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Why Teamwork Matters",
    "section": "",
    "text": "“If everyone is moving forward together, then success takes care of itself.” - Henry Ford"
  },
  {
    "objectID": "posts/difference/index.html",
    "href": "posts/difference/index.html",
    "title": "Why Migrate to the Cloud?",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\nIntroduction\nCloud computing is no longer a foreign concept - we interact with it daily through services like Google Drive, Dropbox, and Office 365. These familiar tools represent just the surface of what cloud technology offers to businesses and organizations today.\n\n\n\nWhat is Cloud Computing?\nCloud computing delivers technology services—including computing power, storage, databases, and networking—over the internet with a flexible pay-as-you-go pricing model. This approach fundamentally changes how organizations acquire and manage their IT resources.\n\n\nStrategic Advantages of Cloud Migration\n\nOperational Excellence\n\nScalability: Dynamically adjust resources to match demand without capital expenditure on hardware\n\nSpeed: Deploy new applications and services in minutes rather than weeks or months\n\nReliability: Distribute data across multiple geographic regions, eliminating single points of failure\n\nSecurity: Leverage enterprise-grade security measures and compliance certifications from major providers\n\n\n\nBusiness Impact\n\nCost Efficiency: Convert capital expenses to operational expenses with predictable pricing models\n\nInnovation Acceleration: Access cutting-edge technologies without significant upfront investment\n\nGlobal Reach: Deploy resources closer to end-users worldwide with minimal effort\n\nFocus on Core Business: Reduce time spent managing infrastructure to concentrate on strategic initiatives\n\n\n\n\nService Models Explained\nCloud computing offers multiple service models that provide different levels of control and responsibility:\n\n\n\n\n\n\n\n\n\nModel\nTraditional Analogy\nWhat You Manage\nWhat Provider Manages\n\n\n\n\nOn-Premises\nOwning a car\nEverything (hardware, software, maintenance)\nNothing\n\n\nIaaS (Infrastructure as a Service)\nRenting a car\nOS, applications, data\nHardware, networking\n\n\nPaaS (Platform as a Service)\nUsing rideshare (Uber)\nApplications, data\nHardware, OS, middleware\n\n\nSaaS (Software as a Service)\nTaking public transit\nData and access\nEverything else\n\n\n\n\n\nImplementation Roadmap\n\n1. Select a Cloud Deployment Model\n\nPublic Cloud: Resources owned and operated by third-party providers\n\nPrivate Cloud: Dedicated resources for a single organization\n\nHybrid Cloud: Combination of public and private clouds with orchestration between them\n\nMulti-Cloud: Strategic use of multiple cloud providers for different services\n\n\n\n2. Develop Necessary Expertise\n\nRoles to Consider: Cloud architects, cloud engineers, DevOps specialists, security experts\n\nSkills to Acquire: Cloud provider certifications, infrastructure as code, containerization, orchestration\n\n\n\n3. Choose Appropriate Provider(s)\n\nMajor Players: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)\n\nSelection Criteria: Service offerings, pricing models, geographic presence, integration capabilities\n\n\n\n4. Create a Comprehensive Migration Strategy\n\nAssessment: Inventory current applications and infrastructure\n\nPlanning: Determine migration approach (rehost, refactor, rearchitect, rebuild)\n\nExecution: Implement the migration with minimal disruption\n\nOptimization: Continuously improve cloud resources and configurations\n\n\n\n\n\nConclusion\nCloud migration has evolved from a technological trend to a business imperative. The combination of financial benefits, operational improvements, and innovation potential makes cloud adoption essential for organizations seeking to remain competitive in the digital landscape.\nBy strategically implementing cloud computing services, companies can enhance agility, improve efficiency, and accelerate innovation while simultaneously reducing technical debt and operational costs. This transformation allows organizations to redirect focus toward their core business objectives and customer needs rather than infrastructure management.\nThe journey to the cloud requires careful planning and execution, but the long-term benefits far outweigh the initial investment in time and resources. As demonstrated in my project, a well-designed migration strategy provides a clear pathway to realizing the full potential of cloud computing."
  },
  {
    "objectID": "posts/difference/index.html#data-engineers-building-the-foundation",
    "href": "posts/difference/index.html#data-engineers-building-the-foundation",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Engineers are the architects who design, build, and maintain the infrastructure that allows data to flow throughout an organization. They create the pipelines that transport data from various sources to storage systems where it can be accessed and analyzed.\n\n\n\n\nProgramming: Proficiency in languages like Python, Java, or Scala\nDatabase Systems: Expert knowledge of SQL and NoSQL databases\nBig Data Technologies: Experience with tools like Hadoop, Spark, and Kafka\nETL Processes: Building robust Extract, Transform, Load pipelines\nCloud Platforms: Working with AWS, Azure, or Google Cloud services\n\n\n\n\nA typical day for a Data Engineer might involve:\n- Designing database schemas\n- Optimizing data flows for performance\n- Implementing data quality checks\n- Setting up data warehousing solutions\n- Creating APIs for data access\n\n\n\nMost Data Engineers come from software engineering or computer science backgrounds and have strong technical foundations in system architecture."
  },
  {
    "objectID": "posts/difference/index.html#data-analysts-interpreting-the-present",
    "href": "posts/difference/index.html#data-analysts-interpreting-the-present",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Analysts focus on examining existing data to answer specific business questions and provide actionable insights. They translate data into information that guides decision-making.\n\n\n\n\nStatistical Analysis: Understanding descriptive statistics\nData Visualization: Creating charts and dashboards\nSQL: Writing queries to extract and manipulate data\nBusiness Intelligence Tools: Proficiency with Tableau, Power BI, or Looker\nSpreadsheet Mastery: Advanced Excel or Google Sheets techniques\n\n\n\n\nA typical day for a Data Analyst might involve: - Creating performance reports - Building interactive dashboards - Conducting A/B test analysis - Identifying trends in customer behavior - Presenting findings to stakeholders\n\n\n\nData Analysts often come from diverse backgrounds including business, economics, mathematics, or even self-taught paths with strong analytical mindsets."
  },
  {
    "objectID": "posts/difference/index.html#data-scientists-predicting-the-future",
    "href": "posts/difference/index.html#data-scientists-predicting-the-future",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Data Scientists combine advanced statistical methods, machine learning, and domain expertise to extract deeper insights, make predictions, and solve complex problems with data.\n\n\n\n\nAdvanced Statistics: Hypothesis testing, experimental design\nMachine Learning: Building predictive models\nProgramming: Strong coding skills in Python or R\nData Storytelling: Communicating complex findings\nDomain Expertise: Understanding business context\n\n\n\n\nA typical day for a Data Scientist might involve: - Developing predictive algorithms - Training and evaluating ML models - Conducting exploratory data analysis - Testing hypotheses with statistical methods - Researching new analytical approaches\n\n\n\nData Scientists typically have advanced degrees in quantitative fields like statistics, mathematics, computer science, or specialized data science programs."
  },
  {
    "objectID": "posts/difference/index.html#how-these-roles-interact",
    "href": "posts/difference/index.html#how-these-roles-interact",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "In an efficient data organization, these three roles form a symbiotic relationship:\n\nData Engineers build the infrastructure and pipelines that collect and organize raw data\nData Scientists leverage the same infrastructure to build models that predict future outcomes\nData Analysts use this data to explain current business performance and trends"
  },
  {
    "objectID": "posts/difference/index.html#career-progression",
    "href": "posts/difference/index.html#career-progression",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "It’s worth noting that career paths in data can be fluid:\n\nSome Data Analysts develop programming and ML skills to become Data Scientists\nSome Data Scientists move into Data Engineering to better understand the full data stack\nOthers specialize further within their domain, becoming experts in specific industries or techniques"
  },
  {
    "objectID": "posts/difference/index.html#salary-comparisons-as-of-2024",
    "href": "posts/difference/index.html#salary-comparisons-as-of-2024",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "Role\nEntry-Level\nMid-Career\nSenior\n\n\n\n\nData Engineer\n$85,000-$110,000\n$110,000-$140,000\n$140,000-$180,000+\n\n\nData Analyst\n$65,000-$85,000\n$85,000-$115,000\n$115,000-$145,000+\n\n\nData Scientist\n$90,000-$120,000\n$120,000-$150,000\n$150,000-$200,000+\n\n\n\n*Note: Salaries vary significantly based on location, industry, company size, and individual experience."
  },
  {
    "objectID": "posts/difference/index.html#conclusion",
    "href": "posts/difference/index.html#conclusion",
    "title": "Data Engineer, Data Scientist, Data Analyst… What’s the Difference?",
    "section": "",
    "text": "While all three roles work with data, they serve complementary functions in the data value chain:\n\nData Engineers focus on the infrastructure that makes data available\nData Scientists focus on models that predict what will happen next\nData Analysts focus on the insights that explain what’s happening now\n\nUnderstanding these distinctions can help organizations build balanced data teams and help individuals chart their career paths in this rapidly evolving field."
  },
  {
    "objectID": "posts/teamwork/index.html",
    "href": "posts/teamwork/index.html",
    "title": "Project Overview",
    "section": "",
    "text": "Our organization recently completed a strategic data platform migration from an on-premise SQL Server environment to a modern cloud-based analytics ecosystem centered on Azure Databricks. This transformation has enabled us to leverage advanced data processing capabilities, implement a robust data lakehouse architecture, and deliver more powerful business insights through enhanced visualization.\n\n\n\nOur solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface\n\n\n\n\nThe end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information\n\n\n\n\n\nThis migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models\n\n\n\n\n\nOur implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance\n\n\n\n\nThe successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/workflow/post.html",
    "href": "posts/workflow/post.html",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "In the world of data analysis, having a solid workflow isn’t just a nice-to-have—it’s your secret weapon for delivering exceptional results. A well-crafted workflow helps you:\n\nSet crystal-clear expectations for your project\nDefine and measure key outcomes with precision\nBoost your accuracy and efficiency\nConsistently deliver high-quality, insightful work\n\nEvery project is unique, and so is every workflow. But one thing remains constant: having a structured approach is always better than flying blind.\n\n\n\n\n\nBefore diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity\nComposition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities\n\n\n\n\n\nLet’s walk through this workflow with a practical example: analyzing customer churn for an e-commerce company.\n\n\nScenario: The marketing director approaches you concerned about declining customer retention rates.\nKey Questions Addressed:\n- Business goal: Reduce customer churn by 15% within 6 months\n- Stakeholders: Marketing, Sales, and Product teams\n- Motivation: Customer acquisition costs are 5x higher than retention costs\n- Strategic alignment: Company’s annual goal to increase customer lifetime value\n\n\n\nDefined Success Metrics:\n- Primary KPI: 30-day customer churn rate\n- Secondary KPIs: Repeat purchase rate, average time between purchases, cart abandonment rate\n- Data needed: Transaction history, customer demographics, website behavior, support interactions\n\n\n\nActions Taken:\n- Extracted 18 months of transaction data from the CRM\n- Merged with customer profile data from the marketing database\n- Cleaned inconsistent date formats and duplicate customer IDs\n- Created feature for “days since last purchase” and “purchase frequency”\n- Automated weekly data refresh using SQL stored procedures\n\n\n\nKey Discoveries:\n- Data covered 45,000 customers and 120,000 transactions\n- Granularity: Individual transaction level with timestamps\n- Identified seasonal purchasing patterns\n- Recognized that mobile app users had different behavior patterns than website users\n\n\n\nApproach:\n- Created cohort analysis showing retention rates by acquisition channel\n- Developed a churn prediction model using purchase recency, frequency, and monetary value\n- Visualized customer journey maps highlighting dropout points\n- Built an interactive dashboard showing churn trends by customer segment\n\n\n\nKey Findings:\n- 64% of churned customers abandoned their shopping cart in their last session\n- Customers who contacted support were 3x more likely to churn\n- First-time customers who purchased during a major sale had 40% higher churn rates\n- Recommendation: Implement targeted retention campaigns for high-risk segments with personalized offers based on previous purchase history\n\n\n\nResults:\n- A/B tested email re-engagement campaigns with 10% of at-risk customers\n- Most effective campaign reduced churn by 22% in the test group\n- Full implementation projected to save $250,000 annually in customer acquisition costs\n- Continuing to refine customer segmentation based on new behavioral data\n\n\n\n\nRemember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value.\nBy embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change. The e-commerce churn example demonstrates how following a structured workflow transforms raw data into valuable business outcomes—precisely what separates exceptional analysts from the rest."
  },
  {
    "objectID": "posts/workflow/post.html#the-7-step-data-analyst-workflow",
    "href": "posts/workflow/post.html#the-7-step-data-analyst-workflow",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Before diving into spreadsheets and data points, take a step back and think like a business owner. Your first mission? Gain a comprehensive understanding of the bigger picture. Ask yourself critical questions:\n\nWhat specific outcomes are we trying to impact?\nWho are the key stakeholders?\nWhat motivates these stakeholders?\nHow does my analysis fit into the broader organizational strategy?\n\nThis upfront clarity helps you align project requirements, define scope, and set meaningful desired outcomes from day one.\n\n\n\nThink of your measurement plan as a treasure map leading to business success. It’s about defining what victory looks like and charting the course to get there. Key elements of an effective measurement plan include:\n\nDefining successful outcomes for the business\nIdentifying relevant Key Performance Indicators (KPIs)\nDetermining the data needed to track and optimize these metrics\n\nPro tip: Skipping this step is like setting sail without a compass. Don’t do it!\n\n\n\nHere’s a hard truth: An analysis is only as powerful as the data supporting it. Always remember: “Garbage in, garbage out”. Data preparation is often the most challenging stage of the workflow, involving:\n\nRigorous quality assurance\nComprehensive data profiling\nStrategic feature engineering\nEfficient ETL (Extract, Transform, Load) automation\n\nConsider this stage your project’s foundation. Invest time here to ensure you’re working with clean, high-quality data.\n\n\n\nBefore you start analyzing, you must truly know your data. This means gaining a crystal-clear understanding of:\n\nData scope\nGranularity\nComposition\nRelevant industry or domain context\n\nThis deep understanding enables efficient work and ensures you have precisely the right data to support your analysis.\n\n\n\nData visualization is more than just creating pretty charts—it’s about translating complex information into clear, compelling narratives. Humans struggle to interpret raw data, so visualization provides the crucial translation. Your goal? Create visual cues that help stakeholders quickly understand complex patterns and insights.\n\n\n\nHere’s a critical reminder: You’re not paid to analyze data. You’re paid to deliver meaningful insights and drive business outcomes. A powerful insight does two things:\n\nTells a clear, data-driven story\nProvides actionable recommendations that directly impact key business objectives\n\nThis is where many analysts fall short, so make it your differentiator.\n\n\n\nCongratulations! You’ve completed your analysis and delivered actionable recommendations. Now it’s time to prove your impact. Focus on:\n\nTracking pre and post-changes to your KPIs\nQuantifying your work’s impact (ideally in monetary terms)\nIdentifying continuous improvement opportunities"
  },
  {
    "objectID": "posts/workflow/post.html#real-world-example-e-commerce-customer-churn-analysis",
    "href": "posts/workflow/post.html#real-world-example-e-commerce-customer-churn-analysis",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Let’s walk through this workflow with a practical example: analyzing customer churn for an e-commerce company.\n\n\nScenario: The marketing director approaches you concerned about declining customer retention rates.\nKey Questions Addressed:\n- Business goal: Reduce customer churn by 15% within 6 months\n- Stakeholders: Marketing, Sales, and Product teams\n- Motivation: Customer acquisition costs are 5x higher than retention costs\n- Strategic alignment: Company’s annual goal to increase customer lifetime value\n\n\n\nDefined Success Metrics:\n- Primary KPI: 30-day customer churn rate\n- Secondary KPIs: Repeat purchase rate, average time between purchases, cart abandonment rate\n- Data needed: Transaction history, customer demographics, website behavior, support interactions\n\n\n\nActions Taken:\n- Extracted 18 months of transaction data from the CRM\n- Merged with customer profile data from the marketing database\n- Cleaned inconsistent date formats and duplicate customer IDs\n- Created feature for “days since last purchase” and “purchase frequency”\n- Automated weekly data refresh using SQL stored procedures\n\n\n\nKey Discoveries:\n- Data covered 45,000 customers and 120,000 transactions\n- Granularity: Individual transaction level with timestamps\n- Identified seasonal purchasing patterns\n- Recognized that mobile app users had different behavior patterns than website users\n\n\n\nApproach:\n- Created cohort analysis showing retention rates by acquisition channel\n- Developed a churn prediction model using purchase recency, frequency, and monetary value\n- Visualized customer journey maps highlighting dropout points\n- Built an interactive dashboard showing churn trends by customer segment\n\n\n\nKey Findings:\n- 64% of churned customers abandoned their shopping cart in their last session\n- Customers who contacted support were 3x more likely to churn\n- First-time customers who purchased during a major sale had 40% higher churn rates\n- Recommendation: Implement targeted retention campaigns for high-risk segments with personalized offers based on previous purchase history\n\n\n\nResults:\n- A/B tested email re-engagement campaigns with 10% of at-risk customers\n- Most effective campaign reduced churn by 22% in the test group\n- Full implementation projected to save $250,000 annually in customer acquisition costs\n- Continuing to refine customer segmentation based on new behavioral data"
  },
  {
    "objectID": "posts/workflow/post.html#conclusion",
    "href": "posts/workflow/post.html#conclusion",
    "title": "The Importance of a Workflow",
    "section": "",
    "text": "Remember, a great workflow isn’t about perfection—it’s about continuous learning and refinement. Each project is a chance to improve your approach and deliver more value.\nBy embracing these workflow principles, you’ll transform from a data analyst into a strategic business partner who drives real, measurable change. The e-commerce churn example demonstrates how following a structured workflow transforms raw data into valuable business outcomes—precisely what separates exceptional analysts from the rest."
  },
  {
    "objectID": "posts/teamwork/index.html#executive-summary",
    "href": "posts/teamwork/index.html#executive-summary",
    "title": "Project Overview",
    "section": "",
    "text": "Our organization recently completed a strategic data platform migration from an on-premise SQL Server environment to a modern cloud-based analytics ecosystem centered on Azure Databricks. This transformation has enabled us to leverage advanced data processing capabilities, implement a robust data lakehouse architecture, and deliver more powerful business insights through enhanced visualization."
  },
  {
    "objectID": "posts/teamwork/index.html#project-architecture",
    "href": "posts/teamwork/index.html#project-architecture",
    "title": "Project Overview",
    "section": "",
    "text": "Our solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface"
  },
  {
    "objectID": "posts/teamwork/index.html#data-flow-and-processing",
    "href": "posts/teamwork/index.html#data-flow-and-processing",
    "title": "Project Overview",
    "section": "",
    "text": "The end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information"
  },
  {
    "objectID": "posts/teamwork/index.html#business-benefits",
    "href": "posts/teamwork/index.html#business-benefits",
    "title": "Project Overview",
    "section": "",
    "text": "This migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models"
  },
  {
    "objectID": "posts/teamwork/index.html#technical-implementation-highlights",
    "href": "posts/teamwork/index.html#technical-implementation-highlights",
    "title": "Project Overview",
    "section": "",
    "text": "Our implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance"
  },
  {
    "objectID": "posts/teamwork/index.html#conclusion",
    "href": "posts/teamwork/index.html#conclusion",
    "title": "Project Overview",
    "section": "",
    "text": "The successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/project overview/index.html",
    "href": "posts/project overview/index.html",
    "title": "Project Overview",
    "section": "",
    "text": "Our solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface\n\n\n\n\nThe end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information\n\n\n\n\n\nThis migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models\n\n\n\n\n\nOur implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance\n\n\n\n\nThe successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/project overview/index.html#project-architecture",
    "href": "posts/project overview/index.html#project-architecture",
    "title": "Project Overview",
    "section": "",
    "text": "Our solution architecture consists of six key components working in harmony:\n\nOn-Premise SQL Server - Source database containing our operational data\nAzure Data Factory - ETL pipeline orchestrator managing data movement\nAzure Data Lake Storage Gen2 (ADLS Gen2) - Raw data storage (Bronze Layer)\nDatabricks - Managed data processing platform (Bronze, Silver, Gold layers)\nAzure Key Vault - Secure credentials and secrets management\nPower BI - Data visualization and reporting interface"
  },
  {
    "objectID": "posts/project overview/index.html#data-flow-and-processing",
    "href": "posts/project overview/index.html#data-flow-and-processing",
    "title": "Project Overview",
    "section": "",
    "text": "The end-to-end data flow follows a modern medallion architecture:\n\n\n\nAzure Data Factory pipelines connect to on-premise SQL Server via secure integration runtime\nIncremental data extraction based on modified timestamps minimizes network traffic\nRaw data lands in ADLS Gen2 storage in the Bronze layer with original schema preserved\n\n\n\n\n\nDatabricks processes data through a three-tier medallion architecture:\n\nBronze Layer: Raw data as extracted from source systems\nSilver Layer: Cleansed, conformed, and validated data\nGold Layer: Business-level aggregates and enriched datasets optimized for analytics\n\n\n\n\n\n\nAzure Key Vault securely stores and manages all credentials and connection strings\nRole-based access control (RBAC) implemented at each layer\nData lineage tracked throughout the pipeline\n\n\n\n\n\nPower BI connects directly to Databricks tables in the Gold layer\nInteractive dashboards provide business users with self-service analytics capabilities\nScheduled refresh ensures reports contain up-to-date information"
  },
  {
    "objectID": "posts/project overview/index.html#business-benefits",
    "href": "posts/project overview/index.html#business-benefits",
    "title": "Project Overview",
    "section": "",
    "text": "This migration has delivered several significant business advantages:\n\nEnhanced Analytics Capabilities\n\nAdvanced data transformations using Spark SQL and Python\nMachine learning integration potential for predictive analytics\nReal-time data processing capabilities\n\nImproved Performance\n\nQuery performance increased by 60% compared to previous environment\nScalable compute resources adjust automatically to workload demands\nParallel processing handles larger data volumes efficiently\n\nCost Optimization\n\nPay-for-use pricing model reduces overall infrastructure costs\nElimination of on-premise hardware refresh cycles\nReduced operational overhead for database administration\n\nFuture-Ready Platform\n\nArchitecture supports seamless integration of new data sources\nFramework established for incorporating unstructured data\nFoundation for implementing machine learning models"
  },
  {
    "objectID": "posts/project overview/index.html#technical-implementation-highlights",
    "href": "posts/project overview/index.html#technical-implementation-highlights",
    "title": "Project Overview",
    "section": "",
    "text": "Our implementation followed industry best practices:\n\nInfrastructure as Code: All resources deployed using Azure ARM templates for consistency\nCI/CD Integration: Automated deployment pipelines for Databricks notebooks and Data Factory pipelines\nMonitoring: Comprehensive logging and alerting using Azure Monitor\nDelta Lake Format: Implementation of transaction support and time travel capabilities\nOptimized Storage: Data partitioning and Z-ordering for query performance"
  },
  {
    "objectID": "posts/project overview/index.html#conclusion",
    "href": "posts/project overview/index.html#conclusion",
    "title": "Project Overview",
    "section": "",
    "text": "The successful migration from our legacy on-premise SQL Server to a modern cloud analytics platform represents a significant milestone in our data transformation journey. This new foundation not only addresses our current analytical needs but positions us for future growth with a flexible, scalable architecture that can evolve with our business requirements.\nWith data now flowing seamlessly from operational systems through our lakehouse architecture to insightful visualizations, business stakeholders can make more informed decisions based on trusted, timely data."
  },
  {
    "objectID": "posts/Why Cloud/index.html",
    "href": "posts/Why Cloud/index.html",
    "title": "Why Migrate to the Cloud?",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\nIntroduction\nCloud computing is no longer a foreign concept - we interact with it daily through services like Google Drive, Dropbox, and Office 365. These familiar tools represent just the surface of what cloud technology offers to businesses and organizations today.\n\n\n\nWhat is Cloud Computing?\nCloud computing delivers technology services—including computing power, storage, databases, and networking—over the internet with a flexible pay-as-you-go pricing model. This approach fundamentally changes how organizations acquire and manage their IT resources.\n\n\nStrategic Advantages of Cloud Migration\n\nOperational Excellence\n\nScalability: Dynamically adjust resources to match demand without capital expenditure on hardware\n\nSpeed: Deploy new applications and services in minutes rather than weeks or months\n\nReliability: Distribute data across multiple geographic regions, eliminating single points of failure\n\nSecurity: Leverage enterprise-grade security measures and compliance certifications from major providers\n\n\n\nBusiness Impact\n\nCost Efficiency: Convert capital expenses to operational expenses with predictable pricing models\n\nInnovation Acceleration: Access cutting-edge technologies without significant upfront investment\n\nGlobal Reach: Deploy resources closer to end-users worldwide with minimal effort\n\nFocus on Core Business: Reduce time spent managing infrastructure to concentrate on strategic initiatives\n\n\n\n\nService Models Explained\nCloud computing offers multiple service models that provide different levels of control and responsibility:\n\n\n\n\n\n\n\n\n\nModel\nTraditional Analogy\nWhat You Manage\nWhat Provider Manages\n\n\n\n\nOn-Premises\nOwning a car\nEverything (hardware, software, maintenance)\nNothing\n\n\nIaaS (Infrastructure as a Service)\nRenting a car\nOS, applications, data\nHardware, networking\n\n\nPaaS (Platform as a Service)\nUsing rideshare (Uber)\nApplications, data\nHardware, OS, middleware\n\n\nSaaS (Software as a Service)\nTaking public transit\nData and access\nEverything else\n\n\n\n\n\nImplementation Roadmap\n\n\n1. Select a Cloud Deployment Model\n\nPublic Cloud: Resources owned and operated by third-party providers\n\nPrivate Cloud: Dedicated resources for a single organization\n\nHybrid Cloud: Combination of public and private clouds with orchestration between them\n\nMulti-Cloud: Strategic use of multiple cloud providers for different services\n\n\n\n2. Develop Necessary Expertise\n\nRoles to Consider: Cloud architects, cloud engineers, DevOps specialists, security experts\n\nSkills to Acquire: Cloud provider certifications, infrastructure as code, containerization, orchestration\n\n\n\n3. Choose Appropriate Provider(s)\n\nMajor Players: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)\n\nSelection Criteria: Service offerings, pricing models, geographic presence, integration capabilities\n\n\n\n4. Create a Comprehensive Migration Strategy\n\nAssessment: Inventory current applications and infrastructure\n\nPlanning: Determine migration approach (rehost, refactor, rearchitect, rebuild)\n\nExecution: Implement the migration with minimal disruption\n\nOptimization: Continuously improve cloud resources and configurations\n\n\n\n\n\nConclusion\nCloud migration has evolved from a technological trend to a business imperative. The combination of financial benefits, operational improvements, and innovation potential makes cloud adoption essential for organizations seeking to remain competitive in the digital landscape.\nBy strategically implementing cloud computing services, companies can enhance agility, improve efficiency, and accelerate innovation while simultaneously reducing technical debt and operational costs. This transformation allows organizations to redirect focus toward their core business objectives and customer needs rather than infrastructure management.\nThe journey to the cloud requires careful planning and execution, but the long-term benefits far outweigh the initial investment in time and resources. As demonstrated in my project, a well-designed migration strategy provides a clear pathway to realizing the full potential of cloud computing."
  },
  {
    "objectID": "posts/why_cloud/index.html",
    "href": "posts/why_cloud/index.html",
    "title": "Why Migrate to the Cloud?",
    "section": "",
    "text": "Why Migrate to the Cloud?\n\nIntroduction\nCloud computing is no longer a foreign concept - we interact with it daily through services like Google Drive, Dropbox, and Office 365! These familiar tools represent just the surface of what cloud technology offers to businesses and organizations today.\n\n\n\nWhat is Cloud Computing?\nCloud computing delivers technology services—including computing power, storage, databases, and networking—over the internet with a flexible pay-as-you-go pricing model. This approach fundamentally changes how organizations acquire and manage their IT resources.\n\n\nStrategic Advantages of Cloud Migration\n\nOperational Excellence\n\nScalability: Dynamically adjust resources to match demand without capital expenditure on hardware\n\nSpeed: Deploy new applications and services in minutes rather than weeks or months\n\nReliability: Distribute data across multiple geographic regions, eliminating single points of failure\n\nSecurity: Leverage enterprise-grade security measures and compliance certifications from major providers\n\n\n\nBusiness Impact\n\nCost Efficiency: Convert capital expenses to operational expenses with predictable pricing models\n\nInnovation Acceleration: Access cutting-edge technologies without significant upfront investment\n\nGlobal Reach: Deploy resources closer to end-users worldwide with minimal effort\n\nFocus on Core Business: Reduce time spent managing infrastructure to concentrate on strategic initiatives\n\n\n\n\nService Models Explained\nCloud computing offers multiple service models that provide different levels of control and responsibility:\n\n\n\n\n\n\n\n\n\nModel\nTraditional Analogy\nWhat You Manage\nWhat Provider Manages\n\n\n\n\nOn-Premises\nOwning a car\nEverything (hardware, software, maintenance)\nNothing\n\n\nIaaS (Infrastructure as a Service)\nRenting a car\nOS, applications, data\nHardware, networking\n\n\nPaaS (Platform as a Service)\nUsing rideshare (Uber)\nApplications, data\nHardware, OS, middleware\n\n\nSaaS (Software as a Service)\nTaking public transit\nData and access\nEverything else\n\n\n\nCan you see how some of those strategic advantages might come into play? While there are times you will want the flexibility to adjust everything manually, like On-Premise models, having the availability of IaaS, PaaS, and SaaS models as you move into the cloud can be a game changer!\n\n\nImplementation Roadmap\n\n\n1. Select a Cloud Deployment Model\nAs you being, you’ll need to take into account what is right for your situation. Which scenario below makes the most sense for you?\n\nPublic Cloud: Resources owned and operated by third-party providers\n\nPrivate Cloud: Dedicated resources for a single organization\n\nHybrid Cloud: Combination of public and private clouds with orchestration between them\n\nMulti-Cloud: Strategic use of multiple cloud providers for different services\n\n\n\n2. Develop Necessary Expertise\nNow that you know you want to move to the cloud, you’re gonna need the skills or people to make it happen.\n\nRoles to Consider: Cloud architects, cloud engineers, DevOps specialists, security experts\n\nSkills to Acquire: Cloud provider certifications, infrastructure as code, containerization, orchestration\n\n\n\n3. Choose Appropriate Provider(s)\nThe providers will determine alot. While they all offer similar things, there are some differences to them. Consider first what will work for you, then see which provider fits best.\n\nMajor Players: Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP)\n\nSelection Criteria: Service offerings, pricing models, geographic presence, integration capabilities\n\n\n\n4. Create a Comprehensive Migration Strategy\nIt’s time to put the planning into action. Ask questions like, “What is our current situation?” and “What would be the best way to go about this?” to get you on the right track.\n\nAssessment: Inventory current applications and infrastructure\n\nPlanning: Determine migration approach (rehost, refactor, rearchitect, rebuild)\n\nExecution: Implement the migration with minimal disruption\n\nOptimization: Continuously improve cloud resources and configurations\n\n\n\n\n\nConclusion\nCloud migration has evolved from a technological trend to a business imperative. The combination of financial benefits, operational improvements, and innovation potential makes cloud adoption essential for organizations seeking to remain competitive in the digital landscape.\nBy strategically implementing cloud computing services, companies can enhance agility, improve efficiency, and accelerate innovation while simultaneously reducing technical debt and operational costs. This transformation allows organizations to redirect focus toward their core business objectives and customer needs rather than infrastructure management.\nThe journey to the cloud requires careful planning and execution, but the long-term benefits far outweigh the initial investment in time and resources. As demonstrated in my project, a well-designed migration strategy provides a clear pathway to realizing the full potential of cloud computing."
  }
]